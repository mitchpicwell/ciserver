Started by an SCM change
Building in workspace /var/lib/jenkins/jobs/PicwellCI/workspace
 > git rev-parse --is-inside-work-tree # timeout=10
Fetching changes from the remote Git repository
 > git config remote.origin.url https://github.com/mitchpicwell/exprod.git # timeout=10
Fetching upstream changes from https://github.com/mitchpicwell/exprod.git
 > git --version # timeout=10
 > git -c core.askpass=true fetch --tags --progress https://github.com/mitchpicwell/exprod.git +refs/heads/*:refs/remotes/origin/*
 > git rev-parse refs/remotes/origin/master^{commit} # timeout=10
 > git rev-parse refs/remotes/origin/origin/master^{commit} # timeout=10
Checking out Revision cbcf7e32f27e912dfef4c25067ef506de9381869 (refs/remotes/origin/master)
 > git config core.sparsecheckout # timeout=10
 > git checkout -f cbcf7e32f27e912dfef4c25067ef506de9381869
 > git rev-list 32a91f93aaaffdbadf6c99132f8ec1bd9eecd905 # timeout=10
No emails were triggered.
[workspace] $ /bin/sh -xe /tmp/hudson5625098686407174741.sh
+ export SPARK_HOME=/var/lib/spark-1.5.2-bin-hadoop2.6
+ export PYTHONPATH=/var/lib/spark-1.5.2-bin-hadoop2.6/python/:
+ nosetests nosetests.py
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
15/11/12 08:09:06 INFO SparkContext: Running Spark version 1.5.2
15/11/12 08:09:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/11/12 08:09:07 INFO SecurityManager: Changing view acls to: jenkins
15/11/12 08:09:07 INFO SecurityManager: Changing modify acls to: jenkins
15/11/12 08:09:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jenkins); users with modify permissions: Set(jenkins)
15/11/12 08:09:08 INFO Slf4jLogger: Slf4jLogger started
15/11/12 08:09:08 INFO Remoting: Starting remoting
15/11/12 08:09:08 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.0.2.15:49842]
15/11/12 08:09:08 INFO Utils: Successfully started service 'sparkDriver' on port 49842.
15/11/12 08:09:08 INFO SparkEnv: Registering MapOutputTracker
15/11/12 08:09:08 INFO SparkEnv: Registering BlockManagerMaster
15/11/12 08:09:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fc0d2db0-0dbb-4437-9e81-731193a274c2
15/11/12 08:09:08 INFO MemoryStore: MemoryStore started with capacity 534.5 MB
15/11/12 08:09:09 INFO HttpFileServer: HTTP File server directory is /tmp/spark-e8d59d89-5350-40f5-ab96-249f8a4d6876/httpd-3d1dd6fc-30e8-4710-af3a-f5baee87faea
15/11/12 08:09:09 INFO HttpServer: Starting HTTP Server
15/11/12 08:09:09 INFO Utils: Successfully started service 'HTTP file server' on port 57665.
15/11/12 08:09:09 INFO SparkEnv: Registering OutputCommitCoordinator
15/11/12 08:09:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/11/12 08:09:09 INFO SparkUI: Started SparkUI at http://10.0.2.15:4040
15/11/12 08:09:09 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.
15/11/12 08:09:09 INFO Executor: Starting executor ID driver on host localhost
15/11/12 08:09:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54187.
15/11/12 08:09:10 INFO NettyBlockTransferService: Server created on 54187
15/11/12 08:09:10 INFO BlockManagerMaster: Trying to register BlockManager
15/11/12 08:09:10 INFO BlockManagerMasterEndpoint: Registering block manager localhost:54187 with 534.5 MB RAM, BlockManagerId(driver, localhost, 54187)
15/11/12 08:09:10 INFO BlockManagerMaster: Registered BlockManager
15/11/12 08:09:10 INFO MemoryStore: ensureFreeSpace(157248) called with curMem=0, maxMem=560497950
15/11/12 08:09:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 153.6 KB, free 534.4 MB)
15/11/12 08:09:10 INFO MemoryStore: ensureFreeSpace(14276) called with curMem=157248, maxMem=560497950
15/11/12 08:09:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.9 KB, free 534.4 MB)
15/11/12 08:09:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:54187 (size: 13.9 KB, free: 534.5 MB)
15/11/12 08:09:10 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2
15/11/12 08:09:11 INFO FileInputFormat: Total input paths to process : 1
15/11/12 08:09:11 INFO SparkContext: Starting job: count at /var/lib/jenkins/jobs/PicwellCI/workspace/main.py:14
15/11/12 08:09:11 INFO DAGScheduler: Got job 0 (count at /var/lib/jenkins/jobs/PicwellCI/workspace/main.py:14) with 1 output partitions
15/11/12 08:09:11 INFO DAGScheduler: Final stage: ResultStage 0(count at /var/lib/jenkins/jobs/PicwellCI/workspace/main.py:14)
15/11/12 08:09:11 INFO DAGScheduler: Parents of final stage: List()
15/11/12 08:09:11 INFO DAGScheduler: Missing parents: List()
15/11/12 08:09:11 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /var/lib/jenkins/jobs/PicwellCI/workspace/main.py:14), which has no missing parents
15/11/12 08:09:11 INFO MemoryStore: ensureFreeSpace(6128) called with curMem=171524, maxMem=560497950
15/11/12 08:09:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.0 KB, free 534.4 MB)
15/11/12 08:09:11 INFO MemoryStore: ensureFreeSpace(3785) called with curMem=177652, maxMem=560497950
15/11/12 08:09:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.7 KB, free 534.4 MB)
15/11/12 08:09:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:54187 (size: 3.7 KB, free: 534.5 MB)
15/11/12 08:09:11 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861
15/11/12 08:09:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at count at /var/lib/jenkins/jobs/PicwellCI/workspace/main.py:14)
15/11/12 08:09:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
15/11/12 08:09:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 2169 bytes)
15/11/12 08:09:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/11/12 08:09:11 INFO HadoopRDD: Input split: file:/var/lib/jenkins/jobs/PicwellCI/workspace/sparknotes.md:0+3359
15/11/12 08:09:11 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/11/12 08:09:11 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/11/12 08:09:11 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/11/12 08:09:11 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/11/12 08:09:11 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/11/12 08:09:11 INFO PythonRunner: Times: total = 133, boot = 121, init = 11, finish = 1
15/11/12 08:09:11 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2124 bytes result sent to driver
15/11/12 08:09:11 INFO DAGScheduler: ResultStage 0 (count at /var/lib/jenkins/jobs/PicwellCI/workspace/main.py:14) finished in 0.342 s
15/11/12 08:09:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 251 ms on localhost (1/1)
15/11/12 08:09:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/11/12 08:09:11 INFO DAGScheduler: Job 0 finished: count at /var/lib/jenkins/jobs/PicwellCI/workspace/main.py:14, took 0.435807 s
15/11/12 08:09:11 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
15/11/12 08:09:11 INFO DAGScheduler: Stopping DAGScheduler
15/11/12 08:09:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/11/12 08:09:11 INFO MemoryStore: MemoryStore cleared
15/11/12 08:09:11 INFO BlockManager: BlockManager stopped
15/11/12 08:09:11 INFO BlockManagerMaster: BlockManagerMaster stopped
15/11/12 08:09:11 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/11/12 08:09:11 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
15/11/12 08:09:11 INFO SparkContext: Successfully stopped SparkContext
15/11/12 08:09:11 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
15/11/12 08:09:12 INFO SparkContext: Running Spark version 1.5.2
15/11/12 08:09:12 INFO SecurityManager: Changing view acls to: jenkins
15/11/12 08:09:12 INFO SecurityManager: Changing modify acls to: jenkins
15/11/12 08:09:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(jenkins); users with modify permissions: Set(jenkins)
15/11/12 08:09:12 INFO Slf4jLogger: Slf4jLogger started
15/11/12 08:09:12 INFO Remoting: Starting remoting
15/11/12 08:09:12 INFO Utils: Successfully started service 'sparkDriver' on port 58228.
15/11/12 08:09:12 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@localhost:58228]
15/11/12 08:09:12 INFO SparkEnv: Registering MapOutputTracker
15/11/12 08:09:12 INFO SparkEnv: Registering BlockManagerMaster
15/11/12 08:09:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-60fbe960-459f-4fea-8043-56a77457445b
15/11/12 08:09:12 INFO MemoryStore: MemoryStore started with capacity 534.5 MB
15/11/12 08:09:12 INFO HttpFileServer: HTTP File server directory is /tmp/spark-e8d59d89-5350-40f5-ab96-249f8a4d6876/httpd-fdce496f-627c-4c02-8154-8fb0eb2a5922
15/11/12 08:09:12 INFO HttpServer: Starting HTTP Server
15/11/12 08:09:12 INFO Utils: Successfully started service 'HTTP file server' on port 51658.
15/11/12 08:09:12 INFO SparkEnv: Registering OutputCommitCoordinator
15/11/12 08:09:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/11/12 08:09:12 INFO SparkUI: Started SparkUI at http://localhost:4040
15/11/12 08:09:12 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.
15/11/12 08:09:12 INFO Executor: Starting executor ID driver on host localhost
15/11/12 08:09:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50478.
15/11/12 08:09:12 INFO NettyBlockTransferService: Server created on 50478
15/11/12 08:09:12 INFO BlockManagerMaster: Trying to register BlockManager
15/11/12 08:09:12 INFO BlockManagerMasterEndpoint: Registering block manager localhost:50478 with 534.5 MB RAM, BlockManagerId(driver, localhost, 50478)
15/11/12 08:09:12 INFO BlockManagerMaster: Registered BlockManager
15/11/12 08:09:13 INFO MemoryStore: ensureFreeSpace(230632) called with curMem=0, maxMem=560497950
15/11/12 08:09:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 225.2 KB, free 534.3 MB)
15/11/12 08:09:13 INFO MemoryStore: ensureFreeSpace(19788) called with curMem=230632, maxMem=560497950
15/11/12 08:09:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 19.3 KB, free 534.3 MB)
15/11/12 08:09:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:50478 (size: 19.3 KB, free: 534.5 MB)
15/11/12 08:09:13 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2
15/11/12 08:09:13 INFO FileInputFormat: Total input paths to process : 1
15/11/12 08:09:13 INFO SparkContext: Starting job: count at /var/lib/jenkins/jobs/PicwellCI/workspace/main.py:14
15/11/12 08:09:13 INFO DAGScheduler: Got job 0 (count at /var/lib/jenkins/jobs/PicwellCI/workspace/main.py:14) with 1 output partitions
15/11/12 08:09:13 INFO DAGScheduler: Final stage: ResultStage 0(count at /var/lib/jenkins/jobs/PicwellCI/workspace/main.py:14)
15/11/12 08:09:13 INFO DAGScheduler: Parents of final stage: List()
15/11/12 08:09:13 INFO DAGScheduler: Missing parents: List()
15/11/12 08:09:13 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /var/lib/jenkins/jobs/PicwellCI/workspace/main.py:14), which has no missing parents
15/11/12 08:09:13 INFO MemoryStore: ensureFreeSpace(6128) called with curMem=250420, maxMem=560497950
15/11/12 08:09:13 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.0 KB, free 534.3 MB)
15/11/12 08:09:13 INFO MemoryStore: ensureFreeSpace(3786) called with curMem=256548, maxMem=560497950
15/11/12 08:09:13 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.7 KB, free 534.3 MB)
15/11/12 08:09:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:50478 (size: 3.7 KB, free: 534.5 MB)
15/11/12 08:09:13 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861
15/11/12 08:09:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at count at /var/lib/jenkins/jobs/PicwellCI/workspace/main.py:14)
15/11/12 08:09:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
15/11/12 08:09:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 2169 bytes)
15/11/12 08:09:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/11/12 08:09:13 INFO HadoopRDD: Input split: file:/var/lib/jenkins/jobs/PicwellCI/workspace/sparknotes.md:0+3359
15/11/12 08:09:13 INFO PythonRunner: Times: total = 115, boot = 110, init = 5, finish = 0
15/11/12 08:09:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2124 bytes result sent to driver
15/11/12 08:09:13 INFO DAGScheduler: ResultStage 0 (count at /var/lib/jenkins/jobs/PicwellCI/workspace/main.py:14) finished in 0.156 s
15/11/12 08:09:13 INFO DAGScheduler: Job 0 finished: count at /var/lib/jenkins/jobs/PicwellCI/workspace/main.py:14, took 0.171257 s
15/11/12 08:09:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 156 ms on localhost (1/1)
15/11/12 08:09:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/11/12 08:09:13 INFO SparkUI: Stopped Spark web UI at http://localhost:4040
15/11/12 08:09:13 INFO DAGScheduler: Stopping DAGScheduler
15/11/12 08:09:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/11/12 08:09:13 INFO MemoryStore: MemoryStore cleared
15/11/12 08:09:13 INFO BlockManager: BlockManager stopped
15/11/12 08:09:13 INFO BlockManagerMaster: BlockManagerMaster stopped
15/11/12 08:09:13 INFO SparkContext: Successfully stopped SparkContext
15/11/12 08:09:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/11/12 08:09:13 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/11/12 08:09:13 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
.
----------------------------------------------------------------------
Ran 1 test in 8.442s

OK
15/11/12 08:09:14 INFO ShutdownHookManager: Shutdown hook called
15/11/12 08:09:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-e8d59d89-5350-40f5-ab96-249f8a4d6876/pyspark-7cb9b368-930d-4da4-b29d-ef80449f92fa
15/11/12 08:09:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-e8d59d89-5350-40f5-ab96-249f8a4d6876/pyspark-1b307788-c7c7-4d7a-8af0-f878051ce3e3
15/11/12 08:09:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-e8d59d89-5350-40f5-ab96-249f8a4d6876
Email was triggered for: Always
Sending email for trigger: Always
Sending email to: mitch.fiorentini@gmail.com
Connection error sending email, retrying once more in 10 seconds...
Connection error sending email, retrying once more in 10 seconds...
